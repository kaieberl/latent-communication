{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import itertools\n",
    "import sys\n",
    "# Add to sys path parent directory\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from utils.dataloaders.full_dataloaders import DataLoaderMNIST, DataLoaderFashionMNIST, DataLoaderCIFAR10, DataLoaderCIFAR100\n",
    "\n",
    "from utils.sampler import *\n",
    "from optimization.fit_mapping import create_mapping\n",
    "from utils.model import load_model, get_transformations\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clear GPU memory\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def define_dataloader(file, file2, use_test_set=False):\n",
    "    if file.strip(\"_\")[0] != file2.strip(\"_\")[0]:\n",
    "        logging.error(\"The datasets are different\")\n",
    "    # Define the dataloaders\n",
    "    name_dataset, name_model, size_of_the_latent, seed = file.strip(\".pth\").split(\"_\")\n",
    "    augumentation = get_transformations(name_model)\n",
    "    if name_dataset.lower() == \"mnist\":\n",
    "        dataloader = DataLoaderMNIST(transformation=augumentation, batch_size=64, seed=int(seed))\n",
    "    if name_dataset.lower() == \"fmnist\":\n",
    "        dataloader = DataLoaderFashionMNIST(transformation=augumentation,batch_size=64, seed=int(seed))\n",
    "    if name_dataset.lower() == \"cifar10\":\n",
    "        dataloader = DataLoaderCIFAR10(transformation=augumentation,batch_size=64, seed=int(seed))\n",
    "    if name_dataset.lower() == \"cifar100\":\n",
    "        dataloader = DataLoaderCIFAR100(transformation=augumentation,batch_size=64, seed=int(seed))\n",
    "    if use_test_set:\n",
    "        full_dataset_images, full_dataset_labels = dataloader.get_full_test_dataset()\n",
    "    else:\n",
    "        full_dataset_images, full_dataset_labels = dataloader.get_full_train_dataset()\n",
    "    return full_dataset_images, full_dataset_labels, len(np.unique(full_dataset_labels.numpy()))\n",
    "\n",
    "\n",
    "def calculate_and_save_mapping(model1, model2, sampling_strategy, sampled_images, parameters, file1, file2, transformations_database, num_samples, lamda, DEVICE):\n",
    "\n",
    "    name_dataset1, name_model1, size_of_the_latent1, seed1 = file1.strip(\".pth\").split(\"_\")\n",
    "    name_dataset2, name_model2, size_of_the_latent2, seed2 = file2.strip(\".pth\").split(\"_\")\n",
    "\n",
    "    # Set the model to evaluation and sends them to the DEVICE \n",
    "    model1.to(torch.float32).to(DEVICE).eval()\n",
    "    model2.to(torch.float32).to(DEVICE).eval()\n",
    "    # Get latent of the sampled images\n",
    "    latent_left_sampled_equally = model1.get_latent_space(sampled_images)\n",
    "    latent_right_sampled_equally = model2.get_latent_space(sampled_images)\n",
    "    latent_left_sampled_equally = latent_left_sampled_equally.to(torch.float32).cpu().detach().numpy()\n",
    "    latent_right_sampled_equally = latent_right_sampled_equally.to(torch.float32).cpu().detach().numpy()\n",
    "    # Create mapping and visualize\n",
    "    cfg = Config(**parameters)\n",
    "    mapping = create_mapping(cfg, latent_left_sampled_equally, latent_right_sampled_equally, do_print=False)\n",
    "    mapping.fit()\n",
    "    storage_path = f'results/transformations/mapping_files/{name_model2}/'\n",
    "    Path(storage_path).mkdir(parents=True, exist_ok=True)\n",
    "    filename = f\"{file1.strip('.pth')}>{file2.strip('.pth')}>{cfg.mapping}_{num_samples}_{lamda}_{sampling_strategy}\"\n",
    "    mapping.save_results(storage_path +  filename)\n",
    "    transformations_database = pd.concat([transformations_database, pd.DataFrame({\"model1\": [file1], \"model2\": [file2], \"mapping\": [storage_path]})], ignore_index=True)\n",
    "    return transformations_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling convex hull:  24%|██▎       | 17/72 [00:18<00:24,  2.24it/s]                                  GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_3.pth:  29%|██▉       | 21/72 [00:29<01:04,  1.26s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/mariotuci/Documents/latent-communication/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_3>Decouple_50_0_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_3.pth:  31%|███       | 22/72 [00:30<01:00,  1.20s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_3>Decouple_50_0.1_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_3.pth:  31%|███       | 22/72 [00:31<01:00,  1.20s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_3>Decouple_50_0.25_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Sampling convex hull:  31%|███       | 22/72 [00:32<01:00,  1.20s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_3>Decouple_50_0.5_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/mariotuci/Documents/latent-communication/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_1.pth:  35%|███▍      | 25/72 [00:43<01:43,  2.21s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_1>Decouple_100_0_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_1.pth:  36%|███▌      | 26/72 [00:43<01:31,  2.00s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_1>Decouple_100_0.1_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_1.pth:  36%|███▌      | 26/72 [00:44<01:31,  2.00s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_1>Decouple_100_0.25_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Sampling convex hull:  39%|███▉      | 28/72 [00:44<01:10,  1.60s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_1>Decouple_100_0.5_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/mariotuci/Documents/latent-communication/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_3.pth and FMNIST_VAE_16_1.pth:  39%|███▉      | 28/72 [00:55<01:10,  1.60s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_1>Decouple_100_0_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_3.pth and FMNIST_VAE_16_1.pth:  42%|████▏     | 30/72 [00:55<01:53,  2.70s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_1>Decouple_100_0.1_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_3.pth and FMNIST_VAE_16_1.pth:  43%|████▎     | 31/72 [00:56<01:35,  2.34s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_1>Decouple_100_0.25_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Sampling convex hull:  44%|████▍     | 32/72 [00:56<01:19,  1.99s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_1>Decouple_100_0.5_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/mariotuci/Documents/latent-communication/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_1.pth and FMNIST_VAE_16_2.pth:  46%|████▌     | 33/72 [01:07<02:26,  3.76s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_2>Decouple_100_0_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_1.pth and FMNIST_VAE_16_2.pth:  47%|████▋     | 34/72 [01:07<01:53,  3.00s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_2>Decouple_100_0.1_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_1.pth and FMNIST_VAE_16_2.pth:  49%|████▊     | 35/72 [01:08<01:28,  2.39s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_2>Decouple_100_0.25_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Sampling convex hull:  50%|█████     | 36/72 [01:08<01:08,  1.91s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_2>Decouple_100_0.5_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/mariotuci/Documents/latent-communication/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_3.pth and FMNIST_VAE_16_2.pth:  51%|█████▏    | 37/72 [01:19<02:27,  4.20s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_2>Decouple_100_0_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_3.pth and FMNIST_VAE_16_2.pth:  53%|█████▎    | 38/72 [01:19<01:47,  3.17s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_2>Decouple_100_0.1_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_3.pth and FMNIST_VAE_16_2.pth:  54%|█████▍    | 39/72 [01:20<01:20,  2.43s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_2>Decouple_100_0.25_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Sampling convex hull:  56%|█████▌    | 40/72 [01:20<01:00,  1.89s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_2>Decouple_100_0.5_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/mariotuci/Documents/latent-communication/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_1.pth and FMNIST_VAE_16_3.pth:  57%|█████▋    | 41/72 [01:31<02:16,  4.41s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_3>Decouple_100_0_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_1.pth and FMNIST_VAE_16_3.pth:  58%|█████▊    | 42/72 [01:31<01:38,  3.29s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_3>Decouple_100_0.1_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_1.pth and FMNIST_VAE_16_3.pth:  60%|█████▉    | 43/72 [01:32<01:12,  2.49s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_3>Decouple_100_0.25_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Sampling convex hull:  61%|██████    | 44/72 [01:32<00:53,  1.90s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_3>Decouple_100_0.5_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/mariotuci/Documents/latent-communication/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_3.pth:  62%|██████▎   | 45/72 [01:43<01:59,  4.42s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_3>Decouple_100_0_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_3.pth:  64%|██████▍   | 46/72 [01:43<01:25,  3.29s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_3>Decouple_100_0.1_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_3.pth:  65%|██████▌   | 47/72 [01:44<01:01,  2.47s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_3>Decouple_100_0.25_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Sampling convex hull:  67%|██████▋   | 48/72 [01:45<00:45,  1.89s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_3>Decouple_100_0.5_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/mariotuci/Documents/latent-communication/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_1.pth:  68%|██████▊   | 49/72 [01:55<01:42,  4.45s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_1>Decouple_200_0_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_1.pth:  69%|██████▉   | 50/72 [01:56<01:12,  3.32s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_1>Decouple_200_0.1_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_1.pth:  71%|███████   | 51/72 [01:56<00:52,  2.52s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_1>Decouple_200_0.25_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Sampling convex hull:  72%|███████▏  | 52/72 [01:57<00:38,  1.95s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_1>Decouple_200_0.5_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/mariotuci/Documents/latent-communication/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_3.pth and FMNIST_VAE_16_1.pth:  74%|███████▎  | 53/72 [02:07<01:23,  4.40s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_1>Decouple_200_0_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_3.pth and FMNIST_VAE_16_1.pth:  75%|███████▌  | 54/72 [02:08<00:59,  3.29s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_1>Decouple_200_0.1_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_3.pth and FMNIST_VAE_16_1.pth:  76%|███████▋  | 55/72 [02:08<00:42,  2.50s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_1>Decouple_200_0.25_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Sampling convex hull:  78%|███████▊  | 56/72 [02:09<00:30,  1.93s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_1>Decouple_200_0.5_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/mariotuci/Documents/latent-communication/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_1.pth and FMNIST_VAE_16_2.pth:  79%|███████▉  | 57/72 [02:19<01:06,  4.43s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_2>Decouple_200_0_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_1.pth and FMNIST_VAE_16_2.pth:  81%|████████  | 58/72 [02:20<00:46,  3.29s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_2>Decouple_200_0.1_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_1.pth and FMNIST_VAE_16_2.pth:  82%|████████▏ | 59/72 [02:21<00:32,  2.51s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_2>Decouple_200_0.25_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Sampling convex hull:  83%|████████▎ | 60/72 [02:21<00:24,  2.00s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_2>Decouple_200_0.5_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/mariotuci/Documents/latent-communication/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_3.pth and FMNIST_VAE_16_2.pth:  85%|████████▍ | 61/72 [02:31<00:48,  4.44s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_2>Decouple_200_0_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_3.pth and FMNIST_VAE_16_2.pth:  86%|████████▌ | 62/72 [02:32<00:33,  3.34s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_2>Decouple_200_0.1_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_3.pth and FMNIST_VAE_16_2.pth:  88%|████████▊ | 63/72 [02:33<00:22,  2.52s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_2>Decouple_200_0.25_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Sampling convex hull:  89%|████████▉ | 64/72 [02:34<00:15,  1.96s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_3>FMNIST_VAE_16_2>Decouple_200_0.5_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/mariotuci/Documents/latent-communication/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_1.pth and FMNIST_VAE_16_3.pth:  90%|█████████ | 65/72 [02:44<00:31,  4.49s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_3>Decouple_200_0_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_1.pth and FMNIST_VAE_16_3.pth:  92%|█████████▏| 66/72 [02:45<00:20,  3.34s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_3>Decouple_200_0.1_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_1.pth and FMNIST_VAE_16_3.pth:  93%|█████████▎| 67/72 [02:45<00:12,  2.53s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_3>Decouple_200_0.25_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Sampling convex hull:  94%|█████████▍| 68/72 [02:46<00:07,  1.97s/it]                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_1>FMNIST_VAE_16_3>Decouple_200_0.5_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/mariotuci/Documents/latent-communication/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "/Users/mariotuci/miniconda/envs/relreps/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_3.pth:  96%|█████████▌| 69/72 [02:56<00:13,  4.44s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_3>Decouple_200_0_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_3.pth:  97%|█████████▋| 70/72 [02:57<00:06,  3.35s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_3>Decouple_200_0.1_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_3.pth:  99%|█████████▊| 71/72 [02:58<00:02,  2.54s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "  | other params | n/a  | 512   \n",
      "--------------------------------------\n",
      "512       Trainable params\n",
      "0         Non-trainable params\n",
      "512       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_3>Decouple_200_0.25_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "Processing FMNIST_VAE_16_2.pth and FMNIST_VAE_16_3.pth: 100%|██████████| 72/72 [02:58<00:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at  results/transformations/mapping_files/VAE/FMNIST_VAE_16_2>FMNIST_VAE_16_3>Decouple_200_0.5_convex_hull\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_save_mappings = pd.read_csv(\"/results/transformations/mapping_files/transfomations_index.csv\")\n",
    "except:\n",
    "    df_save_mappings = pd.DataFrame(columns=[\"model1\", \"model2\", \"mapping\"])\n",
    "\n",
    "## Here is the part that you have to modify however you want\n",
    "## Define directories where you want to ieratively create the mapping, and then write down the parameters you want to use\n",
    "import os \n",
    "# Set the directories where the models are stored\n",
    "os.chdir('/Users/mariotuci/Documents/latent-communication/')\n",
    "folder1 = \"models/checkpoints/VAE/FMNIST\"\n",
    "folder2 = \"models/checkpoints/VAE/FMNIST\"\n",
    "number_samples = [50, 100, 200]    #[10,50,100,200,300]\n",
    "mapping_list = [\"Decouple\"]\n",
    "lamda_list = [0, 0.25,0.5,0.1]    #[0,0.1, 0.01]\n",
    "use_test_set = False\n",
    "filter1 = '_16_' #write here if you want that the processed files contain this string (example \"_50_\" to only process the files with latent size 50)\n",
    "filter2 = '_16_' #write here if you want that the processed files contain this string (example \"_50_\" to only process the files with latent size 50)\n",
    "recalculate = False #If you want to recalculate the mappings, set this to True\n",
    "use_same_sampling = True #If you want to use the same sampling points wherever possible for all the models, set this to True\n",
    "\n",
    "## this autiomatically creates all teh possible setups with the paramenters and the files you speicified, and sets up the correct dataset\n",
    "files1 = [f for f in os.listdir(folder1) if f.endswith(\".pth\") and filter1 in f]\n",
    "files2 = [f for f in os.listdir(folder2) if f.endswith(\".pth\") and filter2 in f]\n",
    "list_of_files = [(f1, f2) for f1, f2 in itertools.product(files1, files2) if f1 != f2]\n",
    "combinations_parameters = list(itertools.product(number_samples, mapping_list, lamda_list))\n",
    "combinations = list(itertools.product(list_of_files, combinations_parameters))\n",
    "flattened_combinations = [(file1, file2, param1, param2, param3) for ((file1, file2), (param1, param2, param3)) in combinations]\n",
    "\n",
    "# Sort the flattened list by all elements\n",
    "sorted_combinations = sorted(flattened_combinations)\n",
    "pbar = tqdm(sorted_combinations)\n",
    "\n",
    "images, labels, n_classes = define_dataloader(files1[0], files2[0], use_test_set)\n",
    "images = images.type(torch.float32)\n",
    "labels = labels.type(torch.float32)\n",
    "\n",
    "images_sampled_equally_old, labels_sampled_equally_old, images_sampled_drop_outliers_old, labels_sampled_drop_outliers_old, images_sampled_worst_classes_old, labels_sampled_worst_classes_old, images_sampled_best_classes_old, labels_sampled_convex_hull_old = None, None, None, None, None, None, None, None\n",
    "past_num_samples, past_file1, past_file2 = None, None, None\n",
    "init = True\n",
    "\n",
    "# Loop through combinations\n",
    "for file1, file2, num_samples, mapping, lamda in pbar:\n",
    "    parameters = {\"num_samples\": num_samples, \"mapping\": mapping, \"lamda\": lamda} #This is done to go around some hydra stuff (<3 kai)\n",
    "    name_dataset1, name_model1, size_of_the_latent1, seed1 = file1.strip(\".pth\").split(\"_\")\n",
    "    name_dataset2, name_model2, size_of_the_latent2, seed2 = file2.strip(\".pth\").split(\"_\")\n",
    "    model_folder = os.listdir(f'results/transformations/mapping_files/{name_model2}')\n",
    "    pbar.set_description(f\"Processing {file1} and {file2}\")\n",
    "    model1 = load_model(model_name=name_model1, name_dataset=name_dataset1, latent_size=int(size_of_the_latent1), seed=int(seed1), model_path = folder1 + '/' + file1)\n",
    "    model2 = load_model(model_name=name_model2, name_dataset=name_dataset2, latent_size=int(size_of_the_latent2), seed=int(seed2), model_path = folder1 + '/' + file2)\n",
    "\n",
    "    if init:\n",
    "        '''pbar.set_description(\"Sampling equally per class\")\n",
    "        images_sampled_equally, labels_sampled_equally = sample_equally_per_class_images(num_samples, images, labels)     \n",
    "        pbar.set_description(\"Sampling removing outliers\")   \n",
    "        images_sampled_drop_outliers, labels_sampled_drop_outliers = sample_removing_outliers(num_samples, images, labels, model2)\n",
    "        pbar.set_description(\"Sampling worst classes\")\n",
    "        images_sampled_worst_classes, labels_sampled_worst_classes = sample_with_half_worst_classes_images(num_samples, images, labels, model2)\n",
    "        '''\n",
    "        pbar.set_description(\"Sampling convex hull\")\n",
    "        images_sampled_best_classes, labels_sampled_convex_hull = sample_convex_hulls_images(num_samples, images, labels, model1)\n",
    "        init = False\n",
    "    '''if (recalculate) or (not any(str(f\"{file1.strip('.pth')}>{file2.strip('.pth')}>{mapping}_{num_samples}_{lamda}_{'equally'}{ext}\") in os.listdir(f'results/transformations/mapping_files/{name_model2}') for ext in ['.npz', '.npy'])):\n",
    "        if (past_num_samples != num_samples) or (not use_same_sampling):\n",
    "            pbar.set_description(\"Sampling equally per class\")\n",
    "            images_sampled_equally, labels_sampled_equally = sample_equally_per_class_images(num_samples, images, labels)        \n",
    "        df_save_mappings = calculate_and_save_mapping(model1, model2, \"equally\", images_sampled_equally, parameters, file1, file2, df_save_mappings, num_samples, lamda, DEVICE)\n",
    "    if (recalculate) or (not any(str(f\"{file1.strip('.pth')}>{file2.strip('.pth')}>{mapping}_{num_samples}_{lamda}_{'outliers'}{ext}\") in os.listdir(f'results/transformations/mapping_files/{name_model2}') for ext in ['.npz', '.npy'])) or init:\n",
    "        if (past_num_samples != num_samples) or (not use_same_sampling) or (past_file2 != file2) or init:    \n",
    "            pbar.set_description(\"Sampling removing outliers\")\n",
    "            images_sampled_drop_outliers, labels_sampled_drop_outliers = sample_removing_outliers(num_samples, images, labels, model2)\n",
    "        df_save_mappings = calculate_and_save_mapping(model1, model2, \"outliers\", images_sampled_drop_outliers, parameters, file1, file2, df_save_mappings, num_samples, lamda, DEVICE)\n",
    "    if (recalculate) or (not any(str(f\"{file1.strip('.pth')}>{file2.strip('.pth')}>{mapping}_{num_samples}_{lamda}_{'worst_classes'}{ext}\") in os.listdir(f'results/transformations/mapping_files/{name_model2}') for ext in ['.npz', '.npy'])) or init:\n",
    "        if (past_num_samples != num_samples) or (not use_same_sampling) or (past_file2 != file2) or init:\n",
    "            pbar.set_description(\"Sampling worst classes\")\n",
    "            images_sampled_worst_classes, labels_sampled_worst_classes = sample_with_half_worst_classes_images(num_samples, images, labels, model2)\n",
    "        df_save_mappings = calculate_and_save_mapping(model1, model2, \"worst_classes\", images_sampled_worst_classes, parameters, file1, file2, df_save_mappings, num_samples, lamda, DEVICE)\n",
    "    '''\n",
    "    if (recalculate) or (not any(str(f\"{file1.strip('.pth')}>{file2.strip('.pth')}>{mapping}_{num_samples}_{lamda}_{'convex_hull'}{ext}\") in os.listdir(f'results/transformations/mapping_files/{name_model2}') for ext in ['.npz','.pth', '.npy'])):\n",
    "        if (past_num_samples != num_samples) or (not use_same_sampling) or (past_file1 != file1):\n",
    "            pbar.set_description(\"Sampling convex hull\")\n",
    "            images_sampled_best_classes, labels_sampled_convex_hull = sample_convex_hulls_images(num_samples, images, labels, model1, device=DEVICE)\n",
    "        df_save_mappings = calculate_and_save_mapping(model1, model2, \"convex_hull\", images_sampled_best_classes, parameters, file1, file2, df_save_mappings, num_samples, lamda, DEVICE)\n",
    "    '''past_num_samples, past_file1, past_file2 = num_samples, file1, file2\n",
    "    images_sampled_equally_old, labels_sampled_equally_old, images_sampled_drop_outliers_old, labels_sampled_drop_outliers_old, images_sampled_worst_classes_old, labels_sampled_worst_classes_old, images_sampled_best_classes_old, labels_sampled_convex_hull_old = images_sampled_equally, labels_sampled_equally, images_sampled_drop_outliers, labels_sampled_drop_outliers, images_sampled_worst_classes, labels_sampled_worst_classes, images_sampled_best_classes, labels_sampled_convex_hull\n",
    "    '''\n",
    "    past_num_samples, past_file1, past_file2 = num_samples, file1, file2\n",
    "    images_sampled_best_classes_old, labels_sampled_convex_hull_old = images_sampled_best_classes, labels_sampled_convex_hull\n",
    "df_save_mappings.to_csv(\"results/transformations/mapping_files/transfomations_index.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".zeroshot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
