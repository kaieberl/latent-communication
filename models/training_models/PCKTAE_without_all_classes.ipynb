{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from utils.dataloaders.full_dataloaders import DataLoaderMNIST, DataLoaderCIFAR10, DataLoaderCIFAR100, DataLoaderFashionMNIST\n",
    "from models.definitions.PCKTAE import PocketAutoencoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.getLogger('torchvision.datasets').setLevel(logging.ERROR)\n",
    "os.chdir('../../')\n",
    "\n",
    "\n",
    "# Initialize DataFrame with additional columns\n",
    "loss_dataset = pd.DataFrame(columns=['Dataset', 'Model','Latent Size', 'Seed', 'Loss'])\n",
    "\n",
    "# Define the lists\n",
    "dataset = 'FMNIST'\n",
    "size_input = 28\n",
    "channels_input = 1\n",
    "seeds = [1, 2, 3]\n",
    "epochs = [20]\n",
    "batch_sizes = [128]\n",
    "learning_rates = [0.005]\n",
    "latent_sizes = [10, 30, 50]\n",
    "\n",
    "# Create combinations for MNIST and CIFAR datasets separately\n",
    "combinations1 = [seeds, epochs, batch_sizes, learning_rates, latent_sizes[:3]]\n",
    "combinations1 = list(itertools.product(*combinations1))\n",
    "\n",
    "# Combine both sets of combinations\n",
    "combinations = combinations1 \n",
    "\n",
    "# Print the combinations and their count\n",
    "print(combinations, len(combinations))\n",
    "\n",
    "# Example of setting device and augmentations (adjust according to your actual use case)\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "for combo in tqdm(combinations):\n",
    "    seed, epoch, batch_size, learning_rate, latent_size = combo\n",
    "    print(f\"Dataset: {dataset}, Channels input: {channels_input}, Epochs: {epoch}, Batch size: {batch_size}, Learning rate: {learning_rate}, Latent size: {latent_size}\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "#Filter the datloader with specific labels\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download and load the dataset\n",
    "trainset = datasets.FashionMNIST(root=os.getcwd()+'/data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST(root=os.getcwd()+'/data/', download=True, train=False, transform=transform)\n",
    "\n",
    "# Specify the labels you want to keep\n",
    "specific_labels = [0, 1, 2, 3,4]\n",
    "labels_string = '_'.join(map(str, specific_labels))\n",
    "labels_string_model = ''.join(map(str, specific_labels))\n",
    "\n",
    "def filter_by_label(dataset, labels):\n",
    "    indices = [i for i, label in enumerate(dataset.targets) if label in labels]\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "# Filter the datasets\n",
    "trainset_filtered = filter_by_label(trainset, specific_labels)\n",
    "testset_filtered = filter_by_label(testset, specific_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "trainloader_filtered = DataLoader(trainset_filtered, batch_size=64, shuffle=True)\n",
    "testloader_filtered = DataLoader(testset_filtered, batch_size=64, shuffle=True)\n",
    "path_model = f'models/checkpoints/SMALLAE_classes{labels_string}/FMNIST/'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "iterations_ = tqdm(combinations)\n",
    "for seed, num_epochs, batch_size, learning_rate, latent_dim in iterations_:\n",
    "    \n",
    "    config = {\n",
    "        'model_name': f'PCKTAECLASS{labels_string_model}',\n",
    "        'dataset': dataset,\n",
    "        'weight_var': 1,\n",
    "        'weight_mean': 0,\n",
    "        'seed': seed,\n",
    "        'batch_size': batch_size,\n",
    "        'num_epochs': num_epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "        'path': path_model\n",
    "    }\n",
    "    \n",
    "    torch.manual_seed(config['seed'])\n",
    "    model = PocketAutoencoder(hidden_dim=latent_dim, n_input_channels=channels_input, input_size=size_input)\n",
    "    model.to(DEVICE)\n",
    "    optimizer = Adam(model.parameters(), lr=config['learning_rate'], weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        overall_loss = 0\n",
    "        model.train()  # Set the model to training mode\n",
    "        \n",
    "        for batch_idx, x in enumerate(trainloader_filtered):\n",
    "            x = x.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.training_step(x)\n",
    "            overall_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_loss = overall_loss / (len(trainloader_filtered) * batch_size)\n",
    "        new_row = pd.DataFrame({'Dataset': [config['dataset']],\n",
    "                                'Model': [config['model_name']],\n",
    "                                'Seed': [config['seed']],\n",
    "                                'Latent Size': [latent_dim],\n",
    "                                'Loss': [avg_loss]})\n",
    "        loss_dataset = pd.concat([loss_dataset, new_row], ignore_index=True)\n",
    "        iterations_.set_description(f\"Epoch: {epoch}, Loss: {avg_loss}\")\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        test_loss = 0\n",
    "        for x_test in testloader_filtered:\n",
    "            x_test = x_test.to(DEVICE)\n",
    "            test_loss += model.validation_step(x_test).item()\n",
    "        \n",
    "        avg_test_loss = test_loss / (len(testloader_filtered) * batch_size)\n",
    "        scheduler.step(avg_test_loss)  # Update the learning rate based on the test loss\n",
    "        new_row = pd.DataFrame({'Dataset': [config['dataset']],\n",
    "                                'Model': [config['model_name']],\n",
    "                                'Seed': [config['seed']],\n",
    "                                'Latent Size': [latent_dim],\n",
    "                                'Loss': [avg_test_loss]})\n",
    "        loss_dataset = pd.concat([loss_dataset, new_row], ignore_index=True)\n",
    "        iterations_.set_description(f\"Test Loss: {avg_test_loss}\")    \n",
    "    \n",
    "    # Save the model\n",
    "    name = f\"{config['dataset']}_{config['model_name']}_{latent_dim}_{config['seed']}.pth\"\n",
    "    print(name)\n",
    "    path = config['path'] + name\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "loss_dataset.to_csv(f'models/checkpoints/SMALLAE_classes/lossesFMNIST.csv', index=False)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".zeroshot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
