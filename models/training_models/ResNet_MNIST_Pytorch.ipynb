{"cells":[{"cell_type":"markdown","metadata":{"id":"wxJkQXLcYQlP"},"source":["# Training ResNET\n","In this notebook one pretrained ResNet is trained and one ResNet from scratch is trained.\n","The models are saved in models/."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27960,"status":"ok","timestamp":1716205817180,"user":{"displayName":"Hillary H.","userId":"06776921334812731064"},"user_tz":-120},"id":"3O67DncFKbBB","outputId":"692ac9a0-5381-4478-c494-c55fb87941df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","['data', 'latent-communication']\n"]}],"source":["seed=3 # (1,2,3,4,5)\n","model_folder = \"models/\"\n","#Amount google driv\n","from google.colab import drive\n","import os\n","\n","gdrive_path='/content/gdrive/MyDrive/case_study_opti'\n","\n","# This will mount your google drive under 'MyDrive'\n","drive.mount('/content/gdrive', force_remount=True)\n","# In order to access the files in this notebook we have to navigate to the correct folder\n","os.chdir(gdrive_path)\n","# Check manually if all files are present\n","print(sorted(os.listdir()))"]},{"cell_type":"code","source":["!pip install lightning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1WUwE0QGhpOK","executionInfo":{"status":"ok","timestamp":1716205895300,"user_tz":-120,"elapsed":78134,"user":{"displayName":"Hillary H.","userId":"06776921334812731064"}},"outputId":"65885837-acf6-45b0-8b0e-a29bf0b56072"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lightning\n","  Downloading lightning-2.2.4-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n","Requirement already satisfied: fsspec[http]<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n","Collecting lightning-utilities<2.0,>=0.8.0 (from lightning)\n","  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n","Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.25.2)\n","Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.0)\n","Requirement already satisfied: torch<4.0,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.2.1+cu121)\n","Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n","  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.4)\n","Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.11.0)\n","Collecting pytorch-lightning (from lightning)\n","  Downloading pytorch_lightning-2.2.4-py3-none-any.whl (802 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.2/802.2 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2.31.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (67.7.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.14.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<4.0,>=1.13.0->lightning)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<4.0,>=1.13.0->lightning)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<4.0,>=1.13.0->lightning)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<4.0,>=1.13.0->lightning)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<4.0,>=1.13.0->lightning)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<4.0,>=1.13.0->lightning)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch<4.0,>=1.13.0->lightning)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<4.0,>=1.13.0->lightning)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<4.0,>=1.13.0->lightning)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch<4.0,>=1.13.0->lightning)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch<4.0,>=1.13.0->lightning)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=1.13.0->lightning)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning, lightning\n","Successfully installed lightning-2.2.4 lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pytorch-lightning-2.2.4 torchmetrics-1.4.0.post0\n"]}]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"nDpjQr6ceOc0","executionInfo":{"status":"ok","timestamp":1716205895301,"user_tz":-120,"elapsed":19,"user":{"displayName":"Hillary H.","userId":"06776921334812731064"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C3OpFPt8BvcL"},"source":["**ResNet Pytorch implementation for MNIST classification**\n","First we import the required packages."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":16092,"status":"ok","timestamp":1716205911378,"user":{"displayName":"Hillary H.","userId":"06776921334812731064"},"user_tz":-120},"id":"xAKTOMBhZnsO"},"outputs":[],"source":["import torch\n","#Set seed\n","\n","\n","%matplotlib inline\n","import torch.nn as nn\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import torchvision\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","from torchvision import transforms\n","import torch.optim as optim\n","import time\n","import tqdm as tqdm\n","from torch.autograd import Variable\n","import importlib\n","from lightning import LightningModule\n","\n","model_resnet = importlib.import_module('latent-communication.resnet.model_def')\n","utils = importlib.import_module('latent-communication.resnet.utils')"]},{"cell_type":"markdown","metadata":{"id":"eH9o40oiCEAo"},"source":["## **Load Dataset**\n","We can load data from pytorch dataset and preprocess it using *transform* function.\n","\n","Note that the ResNet implemented in torchvision take RGB images as inputs, which has three channels. So, here we repeat the single-channel grey scale digits image three times to fit the torchvision model."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3555,"status":"ok","timestamp":1716205914926,"user":{"displayName":"Hillary H.","userId":"06776921334812731064"},"user_tz":-120},"id":"6dkW6FggBsnQ","outputId":"85d4099d-d068-434b-ac76-34e8537e7b53"},"outputs":[{"output_type":"stream","name":"stdout","text":["60000 10000\n"]}],"source":["transform = transforms.Compose([transforms.ToTensor(),\n","                                # expand channel from 1 to 3 to fit\n","                                # ResNet pretrained model\n","                                transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n","                                ])\n","batch_size = 256\n","\n","data_file_path=\"./data\"\n","# download dataset\n","mnist_train = datasets.MNIST(root=data_file_path, train=True, download=True, transform=transform)\n","mnist_test = datasets.MNIST(root=data_file_path, train=False, download=True, transform=transform)\n","print(len(mnist_train), len(mnist_test))\n","\n","# Load dataset\n","train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size,\n","    shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size,\n","    shuffle=True, num_workers=0)"]},{"cell_type":"markdown","metadata":{"id":"qNPQ35vFDxPW"},"source":["## **Building the model**\n","\n","The torchvision model is pretrained on ImageNet with 1000 classes of output, therefore, the network structure is not suitable for the classification in MNIST dataset."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":931,"status":"ok","timestamp":1716205915850,"user":{"displayName":"Hillary H.","userId":"06776921334812731064"},"user_tz":-120},"id":"v8QIpQbPE_Hl","outputId":"5d876ad3-d069-480b-f1dc-32cb0e426dce"},"outputs":[{"output_type":"stream","name":"stdout","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",")\n"]}],"source":["# print pretrain model structure\n","net = models.resnet18()\n","print(net)"]},{"cell_type":"markdown","metadata":{"id":"a_Hmk1y4GJ33"},"source":["## **Modify Pretrain Model Structure**\n","The main structure of the ResNet can be split into two parts: the feature generator (G) and the classifier (F). The pretrained weights on the feature generator can be reused and a new classifier can be trained to fit the calssfication task in MNIST.\n","\n","In the following codes, *ResNetFeatrueExtractor18* reproduces the feature extraction parts of the ResNet18, with an option to load the pretained model. And *ResClassifier* use a fully connected layer to get 10 class predictions.\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1716205915851,"user":{"displayName":"Hillary H.","userId":"06776921334812731064"},"user_tz":-120},"id":"tQU0tzUdbWid"},"outputs":[],"source":["\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        torch.nn.init.xavier_uniform_(m.weight)\n","    elif classname.find('Linear') != -1:\n","        torch.nn.init.xavier_uniform_(m.weight)\n","    elif classname.find('BatchNorm') != -1:\n","        m.weight.data.normal_(1.0, 0.01)\n","\n","# calculate test accuracy\n","def test_accuracy(data_iter, model):\n","    \"\"\"Evaluate testset accuracy of a model.\"\"\"\n","    acc_sum,n = 0,0\n","    for (imgs, labels) in data_iter:\n","        # send data to the GPU if cuda is availabel\n","        if torch.cuda.is_available():\n","            imgs = imgs.cuda()\n","            labels = labels.cuda()\n","        model.eval()\n","        with torch.no_grad():\n","            labels = labels.long()\n","            acc_sum += torch.sum((torch.argmax(model(imgs), dim=1) == labels)).float()\n","            n += labels.shape[0]\n","    return acc_sum.item()/n"]},{"cell_type":"markdown","metadata":{"id":"flCHdHBv1QhI"},"source":["## **Pre-trained model**"]},{"cell_type":"markdown","metadata":{"id":"jWc_WfomXWeu"},"source":["### Training"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":212206,"status":"ok","timestamp":1716206751514,"user":{"displayName":"Hillary H.","userId":"06776921334812731064"},"user_tz":-120},"id":"OA2mJHVQd_aA","outputId":"11df9c93-75e5-45b4-c01a-c7da5429634a"},"outputs":[{"output_type":"stream","name":"stderr","text":["235it [00:19, 11.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 1, loss 0.0005, train acc 0.962, test acc 0.990, time 22.7 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:20, 11.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 2, loss 0.0001, train acc 0.993, test acc 0.992, time 22.0 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 11.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 3, loss 0.0001, train acc 0.996, test acc 0.992, time 21.4 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 12.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 4, loss 0.0000, train acc 0.996, test acc 0.993, time 20.5 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 12.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 5, loss 0.0000, train acc 0.997, test acc 0.993, time 21.0 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 11.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 6, loss 0.0000, train acc 0.998, test acc 0.994, time 21.1 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 12.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 7, loss 0.0000, train acc 0.999, test acc 0.994, time 20.9 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 12.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 8, loss 0.0000, train acc 0.999, test acc 0.993, time 20.8 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 12.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 9, loss 0.0000, train acc 0.999, test acc 0.994, time 20.8 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 12.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 10, loss 0.0000, train acc 0.999, test acc 0.993, time 20.8 sec\n"]}],"source":["model = model = model_resnet.ResNet(pretrained=True)\n","\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","\n","# setting up optimizer for both feature generator G and classifier F.\n","opt = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0005,momentum=0.9)\n","\n","# loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","for epoch in range(0, 10):\n","    n, start = 0, time.time()\n","    train_l_sum = torch.tensor([0.0], dtype=torch.float32)\n","    train_acc_sum = torch.tensor([0.0], dtype=torch.float32)\n","    for i, (imgs, labels) in tqdm.tqdm(enumerate(iter(train_loader))):\n","        model.train()\n","        imgs = Variable(imgs)\n","        labels = Variable(labels)\n","        # train on GPU if possible\n","        if torch.cuda.is_available():\n","            imgs = imgs.cuda()\n","            labels = labels.cuda()\n","            train_l_sum = train_l_sum.cuda()\n","            train_acc_sum = train_acc_sum.cuda()\n","\n","        opt.zero_grad()\n","        # predicted labels\n","        label_hat = model(imgs)\n","\n","        # loss function\n","        loss= criterion(label_hat, labels)\n","        loss.backward()\n","        opt.step()\n","\n","        # calcualte training error\n","        model.eval()\n","        labels = labels.long()\n","        train_l_sum += loss.float()\n","        train_acc_sum += (torch.sum((torch.argmax(label_hat, dim=1) == labels))).float()\n","        n += labels.shape[0]\n","    test_acc = test_accuracy(iter(test_loader), model)\n","    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\\\n","        % (epoch + 1, train_l_sum/n, train_acc_sum/n, test_acc, time.time() - start))\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":479,"status":"ok","timestamp":1716206751984,"user":{"displayName":"Hillary H.","userId":"06776921334812731064"},"user_tz":-120},"id":"v0ZGY7JaYsgJ"},"outputs":[],"source":["## Save the model\n","torch.save(model.state_dict(), f'latent-communication/resnet/models/pretrained_model_seed{seed}.pth')"]},{"cell_type":"markdown","metadata":{"id":"YTZsV8zd1WHw"},"source":["## **Training without Pre-trained model**"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":211770,"status":"ok","timestamp":1716207229460,"user":{"displayName":"Hillary H.","userId":"06776921334812731064"},"user_tz":-120},"id":"098y1Mwn07qZ","outputId":"4f8a4022-2436-4568-ccd2-b5ad9ec7c05c"},"outputs":[{"output_type":"stream","name":"stderr","text":["235it [00:19, 11.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 1, loss 0.0006, train acc 0.952, test acc 0.986, time 22.0 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:20, 11.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 2, loss 0.0001, train acc 0.989, test acc 0.990, time 22.3 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 11.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 3, loss 0.0001, train acc 0.994, test acc 0.986, time 21.2 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 12.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 4, loss 0.0000, train acc 0.997, test acc 0.989, time 20.5 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 12.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 5, loss 0.0000, train acc 0.998, test acc 0.990, time 20.7 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 11.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 6, loss 0.0000, train acc 0.999, test acc 0.991, time 21.0 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 11.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 7, loss 0.0000, train acc 1.000, test acc 0.989, time 21.1 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 12.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 8, loss 0.0000, train acc 1.000, test acc 0.992, time 20.9 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 12.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 9, loss 0.0000, train acc 1.000, test acc 0.992, time 20.8 sec\n"]},{"output_type":"stream","name":"stderr","text":["235it [00:19, 12.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 10, loss 0.0000, train acc 1.000, test acc 0.992, time 20.9 sec\n"]}],"source":["# setting pretrained to False. The rest is the same\n","\n","seed=3\n","torch.manual_seed(seed)\n","\n","model = model_resnet.ResNet(pretrained=False)\n","\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","\n","opt = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0005,momentum=0.9)\n","criterion = nn.CrossEntropyLoss()\n","\n","for epoch in range(0, 10):\n","    n, start = 0, time.time()\n","    train_l_sum = torch.tensor([0.0], dtype=torch.float32)\n","    train_acc_sum = torch.tensor([0.0], dtype=torch.float32)\n","    for i, (imgs, labels) in tqdm.tqdm(enumerate(iter(train_loader))):\n","        model.train()\n","        imgs = Variable(imgs)\n","        labels = Variable(labels)\n","        if torch.cuda.is_available():\n","            imgs = imgs.cuda()\n","            labels = labels.cuda()\n","            train_l_sum = train_l_sum.cuda()\n","            train_acc_sum = train_acc_sum.cuda()\n","\n","        opt.zero_grad()\n","\n","        label_hat = model(imgs)\n","\n","        # loss function\n","        loss= criterion(label_hat, labels)\n","        loss.backward()\n","        opt.step()\n","\n","\n","        # calcualte training error\n","        model.eval()\n","        labels = labels.long()\n","        train_l_sum += loss.float()\n","        train_acc_sum += (torch.sum((torch.argmax(label_hat, dim=1) == labels))).float()\n","        n += labels.shape[0]\n","    test_acc = test_accuracy(iter(test_loader), model)\n","    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\\\n","        % (epoch + 1, train_l_sum/n, train_acc_sum/n, test_acc, time.time() - start))\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":246,"status":"ok","timestamp":1716207241290,"user":{"displayName":"Hillary H.","userId":"06776921334812731064"},"user_tz":-120},"id":"z-QcJKsraA0z"},"outputs":[],"source":["## Save the model\n","torch.save(model.state_dict(), f'latent-communication/resnet/models/model_seed{seed}.pth')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}