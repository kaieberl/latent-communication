{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from utils.dataloaders.dataloader_mnist_single import DataLoaderMNIST\n",
    "from models.definitions.PocketAutoencoder import PocketAutoencoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "os.chdir('/Users/federicoferoggio/Documents/vs_code/latent-communication')\n",
    "\n",
    "\n",
    "# Initialize DataFrame with additional columns\n",
    "loss_dataset = pd.DataFrame(columns=['Dataset', 'Model', 'Seed', 'Epochs', 'Learning Rate', 'Batch Size', 'Loss'])\n",
    "\n",
    "datasets_list = ['MNIST']\n",
    "seeds = [1, 2, 3, 4]\n",
    "paths = ['models/checkpoints/SMALLAE/MNIST/']\n",
    "dataloader_l = [DataLoaderMNIST]\n",
    "epochs = [5, 20]\n",
    "batch_sizes = [64, 128]\n",
    "learning_rates = [0.01, 0.001]\n",
    "\n",
    "combinations1 = [datasets_list, seeds, paths, dataloader_l, epochs, batch_sizes, learning_rates]\n",
    "combinations1 = list(itertools.product(*combinations1))\n",
    "\n",
    "datasets_list = ['MNIST']\n",
    "seeds = [3, 4]\n",
    "paths = ['models/checkpoints/SMALLAE/MNIST/']\n",
    "dataloader_l = [DataLoaderMNIST]\n",
    "epochs = [1, 10]\n",
    "batch_sizes = [32, 64, 128]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "\n",
    "combinations2 = [datasets_list, seeds, paths, dataloader_l, epochs, batch_sizes, learning_rates]\n",
    "combinations2 = list(itertools.product(*combinations2))\n",
    "\n",
    "combinations = combinations1 + combinations2\n",
    "\n",
    "DEVICE = torch.device(\"mps\")\n",
    "augmentations = [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "print(len(combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, seed, paths, dataloader, epochs, batch_size, learning_rate in tqdm(combinations):\n",
    "    dataloader = dataloader(batch_size=batch_size, transformation=augmentations, seed=seed)\n",
    "    test_loader = dataloader.get_test_loader()\n",
    "    train_loader = dataloader.get_train_loader()\n",
    "    config = {\n",
    "        'model_name': 'SMLLAE',\n",
    "        'dataset': dataset,\n",
    "        'weight_var': 1,\n",
    "        'weight_mean': 0,\n",
    "        'seed': seed,\n",
    "        'batch_size': batch_size,\n",
    "        'num_epochs': epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "        'path': paths\n",
    "    }\n",
    "    \n",
    "    torch.manual_seed(config['seed'])\n",
    "    model = PocketAutoencoder()\n",
    "    model.to(DEVICE)\n",
    "    optimizer = Adam(model.parameters(), lr=config['learning_rate'], weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        overall_loss = 0\n",
    "        model.train()  # Set the model to training mode\n",
    "        \n",
    "        for batch_idx, (x, _) in enumerate(train_loader):\n",
    "            x = x.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.training_step(x)\n",
    "            overall_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_loss = overall_loss / (len(train_loader) * batch_size)\n",
    "        new_row = pd.DataFrame({'Dataset': [config['dataset']],\n",
    "                                'Model': [config['model_name']],\n",
    "                                'Seed': [config['seed']],\n",
    "                                'Epochs': [epoch],\n",
    "                                'Learning Rate': [config['learning_rate']],\n",
    "                                'Batch Size': [config['batch_size']],\n",
    "                                'Loss': [avg_loss]})\n",
    "        loss_dataset = pd.concat([loss_dataset, new_row], ignore_index=True)\n",
    "        print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", avg_loss)\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        test_loss = 0\n",
    "        for x_test, _ in test_loader:\n",
    "            x_test = x_test.to(DEVICE)\n",
    "            test_loss += model.validation_step(x_test).item()\n",
    "        \n",
    "        avg_test_loss = test_loss / (len(test_loader) * batch_size)\n",
    "        scheduler.step(avg_test_loss)  # Update the learning rate based on the test loss\n",
    "        new_row = pd.DataFrame({'Dataset': [config['dataset']],\n",
    "                                'Model': [config['model_name']],\n",
    "                                'Seed': [config['seed']],\n",
    "                                'Epochs': ['Test'],\n",
    "                                'Learning Rate': [config['learning_rate']],\n",
    "                                'Batch Size': [config['batch_size']],\n",
    "                                'Loss': [avg_test_loss]})\n",
    "        loss_dataset = pd.concat([loss_dataset, new_row], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # Save the model\n",
    "    name = f\"{config['dataset']}_{config['model_name']}_{config['learning_rate']}_{config['batch_size']}_{config['num_epochs']}_{config['seed']}.pth\"\n",
    "    print(name)\n",
    "    path = config['path'] + name\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "loss_dataset.to_csv('models/checkpoints/SMALLAE/losses.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m     19\u001b[0m augmentations \u001b[38;5;241m=\u001b[39m [transforms\u001b[38;5;241m.\u001b[39mToTensor(), transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,), (\u001b[38;5;241m0.5\u001b[39m,))]\n\u001b[0;32m---> 21\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoaders(batch_size\u001b[38;5;241m=\u001b[39mbatch_size, transformation\u001b[38;5;241m=\u001b[39maugmentations, seed\u001b[38;5;241m=\u001b[39m\u001b[43mseed\u001b[49m)\n\u001b[1;32m     23\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m dataloader\u001b[38;5;241m.\u001b[39mget_test_loader()\n\u001b[1;32m     24\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m dataloader\u001b[38;5;241m.\u001b[39mget_train_loader()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seed' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import Adam\n",
    "from utils.dataloaders.dataloader_mnist_single import DataLoaderMNIST\n",
    "from models.definitions.PocketAutoencoder import PocketAutoencoder\n",
    "from models.definitions.PocketAutoencoder import PocketAutoencoder\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir(\"/Users/federicoferoggio/Documents/vs_code/latent-communication\")\n",
    "\n",
    "# Assuming you have already defined and initialized test_loader and DEVICE\n",
    "\n",
    "losses_per_class = pd.DataFrame(columns=[\"Model\", \"Class\", \"Loss\"])\n",
    "\n",
    "DataLoaders = DataLoaderMNIST\n",
    "batch_size = 64\n",
    "augmentations = [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "\n",
    "dataloader = DataLoaders(batch_size=batch_size, transformation=augmentations)\n",
    "\n",
    "test_loader = dataloader.get_test_loader()\n",
    "train_loader = dataloader.get_train_loader()\n",
    "\n",
    "for file in os.listdir(\"models/checkpoints/SMALLAE/MNIST/\"):\n",
    "    if file.endswith(\".pth\"):  # Check if the file is a PyTorch model file\n",
    "        # Load the model\n",
    "        model = PocketAutoencoder()\n",
    "        model.load_state_dict(torch.load(\"models/checkpoints/SMALLAE/MNIST/\" + file))\n",
    "        print(\"Model loaded:\", file)\n",
    "\n",
    "        # Iterate through each class (0 to 9)\n",
    "        for n in range(10):\n",
    "            desired_class = n  # Specify the class you want to filter\n",
    "            filtered_samples = []\n",
    "\n",
    "            # Filter samples from the test loader based on the desired class\n",
    "            for data, label in test_loader:\n",
    "                indices = torch.nonzero(label == desired_class, as_tuple=False)\n",
    "                if indices.numel() > 0:\n",
    "                    for idx in indices:\n",
    "                        filtered_samples.append((data[idx], label[idx]))\n",
    "\n",
    "            test_loss_filtered = 0  # Initialize test loss for the current class\n",
    "\n",
    "            with torch.no_grad():  # Disable gradient calculation\n",
    "                for x_test, _ in filtered_samples:\n",
    "                    test_loss_filtered += model.validation_step(x_test).item()\n",
    "\n",
    "            # Calculate average reconstruction loss for the current class\n",
    "            if len(filtered_samples) > 0:\n",
    "                avg_loss = test_loss_filtered / len(filtered_samples)\n",
    "            else:\n",
    "                avg_loss = (\n",
    "                    0  # Handle the case when there are no samples for the current class\n",
    "                )\n",
    "\n",
    "            print(\"\\tTest Loss for class\", n, \":\", avg_loss)\n",
    "            # Concatenate the results to the DataFrame\n",
    "            losses_per_class = pd.concat(\n",
    "                [\n",
    "                    losses_per_class,\n",
    "                    pd.DataFrame({\"Model\": [file], \"Class\": [n], \"Loss\": [avg_loss]}),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "# Save the results to a CSV file\n",
    "losses_per_class.to_csv(\"models/checkpoints/SMALLAE/losses_per_class.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: MNIST_SMLLAE_0.001_128_5_4.pth\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m filtered_samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Filter samples from the test loader based on the desired class\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, label \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_loader\u001b[49m:\n\u001b[1;32m     43\u001b[0m     indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnonzero(label \u001b[38;5;241m==\u001b[39m desired_class, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m indices\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from models.definitions.PocketAutoencoder import PocketAutoencoder\n",
    "\n",
    "# Assuming you have already defined and initialized test_loader and DEVICE\n",
    "\n",
    "os.chdir(\"/Users/federicoferoggio/Documents/vs_code/latent-communication\")\n",
    "losses_per_class = pd.DataFrame(columns=[\"Model\", \"Class\", \"MSE\", \"SSIM\", \"PSNR\"])\n",
    "\n",
    "\n",
    "# Function to calculate SSIM and PSNR\n",
    "def calculate_ssim_psnr(original, reconstructed, data_range=1.0):\n",
    "    original_np = original.cpu().numpy().squeeze()\n",
    "    reconstructed_np = reconstructed.cpu().numpy().squeeze()\n",
    "    ssim_value = ssim(original_np, reconstructed_np, data_range=data_range)\n",
    "    psnr_value = psnr(original_np, reconstructed_np, data_range=data_range)\n",
    "    return ssim_value, psnr_value\n",
    "\n",
    "\n",
    "# Iterate through each model file in the specified directory\n",
    "for file in os.listdir(\"models/checkpoints/SMALLAE/MNIST/\"):\n",
    "    if file.endswith(\".pth\"):  # Check if the file is a PyTorch model file\n",
    "        # Load the model\n",
    "        model = PocketAutoencoder()\n",
    "        model.load_state_dict(\n",
    "            torch.load(\"models/checkpoints/SMALLAE/MNIST/\" + file, map_location=DEVICE),\n",
    "            strict=False,\n",
    "        )\n",
    "        model.to(DEVICE)\n",
    "        print(\"Model loaded:\", file)\n",
    "\n",
    "        # Iterate through each class (0 to 9)\n",
    "        for n in range(10):\n",
    "            desired_class = n  # Specify the class you want to filter\n",
    "            filtered_samples = []\n",
    "\n",
    "            # Filter samples from the test loader based on the desired class\n",
    "            for data, label in test_loader:\n",
    "                indices = torch.nonzero(label == desired_class, as_tuple=False)\n",
    "                if indices.numel() > 0:\n",
    "                    for idx in indices:\n",
    "                        filtered_samples.append((data[idx], label[idx]))\n",
    "\n",
    "            mse_loss_filtered = 0  # Initialize MSE loss for the current class\n",
    "            ssim_loss_filtered = 0  # Initialize SSIM loss for the current class\n",
    "            psnr_loss_filtered = 0  # Initialize PSNR loss for the current class\n",
    "\n",
    "            with torch.no_grad():  # Disable gradient calculation\n",
    "                for x_test, _ in filtered_samples:\n",
    "                    x_test = x_test.to(DEVICE)\n",
    "                    x_reconstructed = model(x_test)\n",
    "                    mse_loss_filtered += torch.nn.functional.mse_loss(\n",
    "                        x_reconstructed, x_test\n",
    "                    ).item()\n",
    "                    ssim_value, psnr_value = calculate_ssim_psnr(\n",
    "                        x_test, x_reconstructed\n",
    "                    )\n",
    "                    ssim_loss_filtered += ssim_value\n",
    "                    psnr_loss_filtered += psnr_value\n",
    "\n",
    "            # Calculate average losses for the current class\n",
    "            num_samples = len(filtered_samples)\n",
    "            if num_samples > 0:\n",
    "                avg_mse_loss = mse_loss_filtered / num_samples\n",
    "                avg_ssim_loss = ssim_loss_filtered / num_samples\n",
    "                avg_psnr_loss = psnr_loss_filtered / num_samples\n",
    "            else:\n",
    "                avg_mse_loss = (\n",
    "                    0  # Handle the case when there are no samples for the current class\n",
    "                )\n",
    "                avg_ssim_loss = 0\n",
    "                avg_psnr_loss = 0\n",
    "\n",
    "            print(\n",
    "                f\"\\tMetrics for class {n} - MSE: {avg_mse_loss}, SSIM: {avg_ssim_loss}, PSNR: {avg_psnr_loss}\"\n",
    "            )\n",
    "            # Concatenate the results to the DataFrame\n",
    "            losses_per_class = pd.concat(\n",
    "                [\n",
    "                    losses_per_class,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"Model\": [file],\n",
    "                            \"Class\": [n],\n",
    "                            \"MSE\": [avg_mse_loss],\n",
    "                            \"SSIM\": [avg_ssim_loss],\n",
    "                            \"PSNR\": [avg_psnr_loss],\n",
    "                        }\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "# Save the results to a CSV file\n",
    "losses_per_class.to_csv(\"models/checkpoints/SMALLAE/MNIST/more_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m order_check \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m pth_files \u001b[38;5;241m=\u001b[39m [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/checkpoints/SMALLAE/MNIST/\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/checkpoints/SMALLAE/MNIST/\u001b[39m\u001b[38;5;124m'\u001b[39m, file)) \u001b[38;5;129;01mand\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(pth_files):\n\u001b[1;32m      6\u001b[0m     model_order \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m     latent_space_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "order_check = []\n",
    "\n",
    "pth_files = [file for file in os.listdir('models/checkpoints/SMALLAE/MNIST/') if os.path.isfile(os.path.join('models/checkpoints/SMALLAE/MNIST/', file)) and file.endswith('.pth')]\n",
    "\n",
    "for file in tqdm(pth_files):\n",
    "    model_order = []\n",
    "    latent_space_dict = {}\n",
    "    model1 = PocketAutoencoder()\n",
    "    model1.load_state_dict(torch.load(f'models/checkpoints/SMALLAE/MNIST/{file}'))\n",
    "    model1.to(DEVICE)\n",
    "    model1.eval()\n",
    "    \n",
    "    DataLoaders = DataLoaderMNIST\n",
    "    indices = range(60000)  # Assuming you want to use the first 40,000 images\n",
    "    data_loader = DataLoaderMNIST(128, augmentations, indices=indices, seed=0)\n",
    "    train_loader = data_loader.get_train_loader()\n",
    "\n",
    "    # Get all images from test_loader and convert them to latent space\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        if batch_idx == len(train_loader) - 1:\n",
    "            break  # Skip the last batch\n",
    "        images = images.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            latent_space = model1.get_latent_space(images)\n",
    "\n",
    "        # Convert tensor to numpy array\n",
    "        latent_space_np = latent_space.detach().cpu().numpy()\n",
    "        labels_np = labels.detach().cpu().numpy()\n",
    "        \n",
    "        # Store in dictionary\n",
    "        for numm, idx in enumerate(range(batch_idx*128, (batch_idx+1)*128)):\n",
    "            latent_space_dict[idx] = (latent_space_np[numm], labels_np[numm])\n",
    "            model_order.append(labels_np[numm])\n",
    "    order_check.append(model_order)\n",
    "    torch.save(latent_space_dict, 'models/checkpoints/SMALLAE/MNIST/LATENTS/' + str(file).replace('.pth', '_latent_space.pth'))\n",
    "\n",
    "## Check they are in the same order\n",
    "for i in range(len(order_check)-1):\n",
    "    if order_check[i] != order_check[i+1]:\n",
    "        print('Order is not the same')\n",
    "        break\n",
    "\n",
    "print(len(order_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".zeroshot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
